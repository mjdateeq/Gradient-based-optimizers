# Gradient-based-optimizers
I have worked on differnt optimizers to test logic networks. Logic networks are Neural networks with a different relizations of neurons
The optimizers are ADAM, AdaMax, Nadam and AMSGRAD. 
